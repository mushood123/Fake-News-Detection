# -*- coding: utf-8 -*-
"""Fake_News_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uis44AtlvRmkXlq66JNugtpdKdZBqdmC
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
import pickle
import re
from scipy.sparse import hstack
from google.colab import drive  # For mounting Google Drive in Colab

# Mounting google drive and loading datasets
drive.mount('/content/drive')
try:
  fake_dataset=pd.read_csv("/content/drive/MyDrive/Fake.csv")
  real_dataset=pd.read_csv("/content/drive/MyDrive/True.csv")
except FileNotFoundError:
  print("Error: File not found. Please check the file path and ensure the file exists in your Google Drive.")

real_dataset.head()

fake_dataset.head()

real_dataset['class'] = ['Real'] * len(real_dataset)
fake_dataset['class'] = ['Fake'] * len(fake_dataset)

fake_dataset.head()

real_dataset.head()

# Excluding Date Column
real_dataset=real_dataset.drop(columns=['date'])
fake_dataset=fake_dataset.drop(columns=['date'])

# Removing Tags or mentioning @
# Also lowering the text
real_dataset['title'] = real_dataset['title'].str.lower().replace(r'[^\w\s]', '', regex=True)
real_dataset['text'] = real_dataset['text'].str.lower().replace(r'[^\w\s]', '', regex=True)

fake_dataset['title'] = fake_dataset['title'].str.lower().replace(r'[^\w\s]', '', regex=True)
fake_dataset['text'] = fake_dataset['text'].str.lower().replace(r'[^\w\s]', '', regex=True)

real_dataset['title']

"""Converting the Textual Data to Numerical Form"""

tfidf_title = TfidfVectorizer(max_features=1000)
tfidf_text = TfidfVectorizer(max_features=1000)

# Fit and transform the 'title' column for real dataset
real_title_features = tfidf_title.fit_transform(real_dataset['title']).toarray()

# Transform the 'title' column for fake dataset using the same fitted vectorizer
fake_title_features = tfidf_title.transform(fake_dataset['title']).toarray()

# Fit and transform the 'text' column for real dataset
real_text_features = tfidf_text.fit_transform(real_dataset['text']).toarray()

# Transform the 'text' column for fake dataset using the same fitted vectorizer
fake_text_features = tfidf_text.transform(fake_dataset['text']).toarray()

# Initialize LabelEncoder for 'subject'
subject_encoder = LabelEncoder()
subject_encoder.fit(pd.concat([fake_dataset['subject'], real_dataset['subject']], ignore_index=True))

# Transform 'subject' column in both datasets
real_subject_features = subject_encoder.transform(real_dataset['subject']).reshape(-1, 1)
fake_subject_features = subject_encoder.transform(fake_dataset['subject']).reshape(-1, 1)

# Encode the 'class' labels (Real/Fake)
class_encoder = LabelEncoder()
class_encoder.fit(pd.concat([fake_dataset['class'], real_dataset['class']], ignore_index=True))

# Transform 'class' column for both datasets
real_class_labels = class_encoder.transform(real_dataset['class'])
fake_class_labels = class_encoder.transform(fake_dataset['class'])

features_real=real_dataset.drop(columns=['class'])
label_real=real_dataset['class']
features_fake=fake_dataset.drop(columns=['class'])
label_fake=fake_dataset['class']

# Create feature matrices for real and fake datasets
features_real = np.concatenate([real_title_features, real_text_features, real_subject_features], axis=1)
features_fake = np.concatenate([fake_title_features, fake_text_features, fake_subject_features], axis=1)

real_dataset

# Split the data (20% for testing, 80% for training)
real_count = int(len(features_real) * 0.20)
fake_count = int(len(features_fake) * 0.20)

# Select the first 20% of records from each dataset for testing
X_test_real = features_real[:real_count]
X_test_fake = features_fake[:fake_count]
y_test_real = real_class_labels[:real_count]
y_test_fake = fake_class_labels[:fake_count]

# Remaining 80% for training
X_train_real = features_real[real_count:]
X_train_fake = features_fake[fake_count:]
y_train_real = real_class_labels[real_count:]
y_train_fake = fake_class_labels[fake_count:]

# Concatenate the training and testing data
X_test = np.concatenate([X_test_real, X_test_fake], axis=0)
y_test = np.concatenate([y_test_real, y_test_fake], axis=0)

X_train = np.concatenate([X_train_real, X_train_fake], axis=0)
y_train = np.concatenate([y_train_real, y_train_fake], axis=0)


# Confirm the sizes
print(f"X_test size: {len(X_test)}")
print(f"y_test size: {len(y_test)}")
print(f"X_train size: {len(X_train)}")
print(f"y_train size: {len(y_train)}")

# Train a model
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate
print(classification_report(y_test, y_pred))

objects_to_save = {
    'subject_encoder': subject_encoder,
    'class_encoder': class_encoder,
    'tfidf_vectorizer_title': tfidf_title,
    'tfidf_vectorizer_text': tfidf_text,
    'model': model
}

# Save the objects using pickle
with open('model_and_preprocessing.pkl', 'wb') as f:
    pickle.dump(objects_to_save, f)

# Load the saved objects
with open('model_and_preprocessing.pkl', 'rb') as f:
    loaded_objects = pickle.load(f)

# Access the individual objects
subject_encoder = loaded_objects['subject_encoder']
class_encoder = loaded_objects['class_encoder']
tfidf_vectorizer_title = loaded_objects['tfidf_vectorizer_title']
tfidf_vectorizer_text = loaded_objects['tfidf_vectorizer_text']
model = loaded_objects['model']

subject_encoder.classes_

def preprocess_tweet(tweet):
    # Lowercase the text
    tweet = tweet.lower()
    # Remove special characters and numbers
    tweet = re.sub(r'[^\w\s]', '', tweet)
    return tweet

# Example data for prediction
# example_data = pd.DataFrame({
#     'title': ['WHO Declares Global Effort to Combat Pandemic'],
#     'text': ['NASA has successfully landed its latest rover on Mars, marking another milestone in space exploration. The rover will study the planets surface for signs of ancient life'],
#     'subject': ['worldnews']
# })
example_data = pd.DataFrame({
    'title': ['Grenfell Tower Fire (2017)'],
    'text': ['In June 2017, a fire engulfed the Grenfell Tower in London, killing 72 people. The tragedy exposed serious failures in building safety regulations and sparked widespread outrage and calls for reform'],
    'subject': ['worldnews']
})

# Preprocess the example data
example_data['title'] = example_data['title'].str.lower().replace(r'[^\w\s]', '', regex=True)
example_data['text'] = example_data['text'].str.lower().replace(r'[^\w\s]', '', regex=True)

# Transform the title and text using the loaded TF-IDF vectorizers
example_data_title_tfidf = tfidf_vectorizer_title.transform(example_data['title']).toarray()
example_data_text_tfidf = tfidf_vectorizer_text.transform(example_data['text']).toarray()

# Transform the subject using the loaded subject encoder
example_data['subject'] = subject_encoder.transform(example_data['subject'])

# Combine the features into a single dataset
example_features = pd.DataFrame(example_data_title_tfidf, columns=[f'title_tfidf_{i}' for i in range(example_data_title_tfidf.shape[1])])
text_features = pd.DataFrame(example_data_text_tfidf, columns=[f'text_tfidf_{i}' for i in range(example_data_text_tfidf.shape[1])])
example_features = pd.concat([example_features, text_features, example_data['subject'].reset_index(drop=True)], axis=1)

# Make predictions
predictions = model.predict(example_features)

# Decode the predictions back to class labels
decoded_predictions = class_encoder.inverse_transform(predictions)

# Output the prediction result
for i, prediction in enumerate(decoded_predictions):
    print(f"Example {i + 1}: Predicted class is '{prediction}'")

